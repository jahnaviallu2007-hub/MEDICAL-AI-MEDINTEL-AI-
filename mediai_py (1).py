# -*- coding: utf-8 -*-
"""MEDIAI.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XC564JT_ZCeQ2e11nHqa8ftGy_4eZ02B
"""

!pip install fpdf2

!apt-get install -y tesseract-ocr
!pip install pytesseract
!pip install fpdf2
!pip install transformers accelerate torch gradio pillow

# ===== MEDI AI — FINAL STABLE VERSION (NO INPUT, NO INTERRUPT) =====

import os, traceback, platform, shutil
from PIL import Image
import pytesseract
from fpdf import FPDF
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# OPTIONAL: Paste your HuggingFace token here (leave empty to use fallback mode)
HF_TOKEN = ""   # example: "hf_xxxxxxxxxxxxxxxxx"

# Safe path for Tesseract
if platform.system() == "Windows":
    pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
elif platform.system() == "Darwin":
    pytesseract.pytesseract.tesseract_cmd = "/opt/homebrew/bin/tesseract"
else:
    pytesseract.pytesseract.tesseract_cmd = shutil.which("tesseract") or "/usr/bin/tesseract"

# Try loading model
USE_REAL_MODEL = False
tokenizer = model = None

def load_model():
    global tokenizer, model, USE_REAL_MODEL
    if HF_TOKEN.strip() == "":
        print("No HuggingFace token provided — using fallback.")
        return

    try:
        model_name = "ibm-granite/granite-3.3-2b-instruct"
        print("Loading Granite model (this takes time)...")

        tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto",
            token=HF_TOKEN
        )
        USE_REAL_MODEL = True
        print("Granite loaded successfully!")
    except Exception as e:
        print("Failed to load Granite model → Using fallback.")
        traceback.print_exc()

load_model()

# AI reply engine
def ai_reply(prompt):
    if USE_REAL_MODEL:
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            out = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
            return tokenizer.decode(out[0], skip_special_tokens=True)
        except Exception:
            traceback.print_exc()

    # Fallback response
    return (
        "MEDI AI (offline mode):\n"
        "Based on your symptoms, here are next steps:\n"
        "- Drink plenty of fluids\n"
        "- Take paracetamol for fever (consult doctor for correct dose)\n"
        "- Monitor symptoms\n"
        "- Visit doctor if symptoms become severe\n\n"
        "Upload reports for better analysis."
    )

# OCR extraction
def extract_text(img):
    try:
        return pytesseract.image_to_string(Image.open(img))
    except Exception:
        traceback.print_exc()
        return ""

# PDF generator
def create_pdf(text):
    filename = "/content/MEDI_AI_REPORT.pdf"
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    for line in text.split("\n"):
        pdf.multi_cell(0, 8, line)

    pdf.output(filename)
    return filename

# Main function
def medi_ai(symptoms, image, medication, age):
    extracted = ""
    pdf_file = None

    if image:
        extracted = extract_text(image)
        pdf_file = create_pdf(extracted)

    prompt = f"""
User Symptoms: {symptoms}
Age: {age}
Already on medication: {medication}
Extracted Report: {extracted}

Provide:
1. Diagnosis clues
2. Additional tests needed
3. Medicines + dosage (safe general advice)
4. Home remedies
5. When to visit a doctor
6. A caring closing message
"""

    reply = ai_reply(prompt)

    return reply, extracted, pdf_file

# Gradio UI
interface = gr.Interface(
    fn=medi_ai,
    inputs=[
        gr.Textbox(label="Symptoms"),
        gr.Image(type="filepath", label="Report / Prescription (optional)"),
        gr.Radio(["Yes", "No"], label="On Medication?"),
        gr.Number(label="Age")
    ],
    outputs=[
        gr.Markdown(label="MEDI AI Response"),
        gr.Textbox(label="Extracted Text (OCR)"),
        gr.File(label="Download PDF")
    ],
    title="MEDI AI — Final Stable Version"
)

interface.launch(debug=True)

# ===== MEDI AI — FINAL STABLE VERSION (NO INPUT, NO INTERRUPT) =====

import os, traceback, platform, shutil
from PIL import Image
import pytesseract
from fpdf import FPDF
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# OPTIONAL: Paste your HuggingFace token here (leave empty to use fallback mode)
HF_TOKEN = ""   # example: "hf_xxxxxxxxxxxxxxxxx"

# Safe path for Tesseract
if platform.system() == "Windows":
    pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
elif platform.system() == "Darwin":
    pytesseract.pytesseract.tesseract_cmd = "/opt/homebrew/bin/tesseract"
else:
    pytesseract.pytesseract.tesseract_cmd = shutil.which("tesseract") or "/usr/bin/tesseract"

# Try loading model
USE_REAL_MODEL = False
tokenizer = model = None

def load_model():
    global tokenizer, model, USE_REAL_MODEL
    if HF_TOKEN.strip() == "":
        print("No HuggingFace token provided — using fallback.")
        return

    try:
        model_name = "ibm-granite/granite-3.3-2b-instruct"
        print("Loading Granite model (this takes time)...")

        tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto",
            token=HF_TOKEN
        )
        USE_REAL_MODEL = True
        print("Granite loaded successfully!")
    except Exception as e:
        print("Failed to load Granite model → Using fallback.")
        traceback.print_exc()

load_model()

# AI reply engine
def ai_reply(prompt):
    if USE_REAL_MODEL:
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            out = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
            return tokenizer.decode(out[0], skip_special_tokens=True)
        except Exception:
            traceback.print_exc()

    # Fallback response
    return (
        "MEDI AI (offline mode):\n"
        "Based on your symptoms, here are next steps:\n"
        "- Drink plenty of fluids\n"
        "- Take paracetamol for fever (consult doctor for correct dose)\n"
        "- Monitor symptoms\n"
        "- Visit doctor if symptoms become severe\n\n"
        "Upload reports for better analysis."
    )

# OCR extraction
def extract_text(img):
    try:
        return pytesseract.image_to_string(Image.open(img))
    except Exception:
        traceback.print_exc()
        return ""

# PDF generator
def create_pdf(text):
    filename = "/content/MEDI_AI_REPORT.pdf"
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    for line in text.split("\n"):
        pdf.multi_cell(0, 8, line)

    pdf.output(filename)
    return filename

# Main function
def medi_ai(symptoms, image, medication, age):
    extracted = ""
    pdf_file = None

    if image:
        extracted = extract_text(image)
        pdf_file = create_pdf(extracted)

    prompt = f"""
User Symptoms: {symptoms}
Age: {age}
Already on medication: {medication}
Extracted Report: {extracted}

Provide:
1. Diagnosis clues
2. Additional tests needed
3. Medicines + dosage (safe general advice)
4. Home remedies
5. When to visit a doctor
6. A caring closing message
"""

    reply = ai_reply(prompt)

    return reply, extracted, pdf_file

# Gradio UI
interface = gr.Interface(
    fn=medi_ai,
    inputs=[
        gr.Textbox(label="Symptoms"),
        gr.Image(type="filepath", label="Report / Prescription (optional)"),
        gr.Radio(["Yes", "No"], label="On Medication?"),
        gr.Number(label="Age")
    ],
    outputs=[
        gr.Markdown(label="MEDI AI Response"),
        gr.Textbox(label="Extracted Text (OCR)"),
        gr.File(label="Download PDF")
    ],
    title="MEDI AI — Final Stable Version"
)

interface.launch(debug=True)